{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "\n",
    "def bootstrap(perfRuntimes1: pd.DataFrame, perfRuntimes2: pd.DataFrame):\n",
    "  numberOfIterations = 5\n",
    "  instanceRunsNumber = 3\n",
    "  instanceRuns = range(1, instanceRunsNumber)\n",
    "  suiteRunsNumber = 3\n",
    "  suiteRuns = range(1,suiteRunsNumber)\n",
    "  numberOfSamples = 10000\n",
    "  allRuntimes1 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "  allRuntimes2 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "\n",
    "  for instanceRun in instanceRuns:\n",
    "    for suiteRun in suiteRuns:\n",
    "        prefix = f\"{instanceRun}-{suiteRun}-\"\n",
    "        allRuntimes1[instanceRun][suiteRun] = perfRuntimes1.loc[(perfRuntimes1['R-S-I'].str.startswith(prefix)),'sec/op'].to_numpy()\n",
    "        allRuntimes2[instanceRun][suiteRun] = perfRuntimes2.loc[(perfRuntimes2['R-S-I'].str.startswith(prefix)),'sec/op'].to_numpy()\n",
    "\n",
    "  #Generate random arrays\n",
    "  currentInstanceRun = rng.choice(instanceRuns, size=(instanceRunsNumber, numberOfSamples))\n",
    "  currentSuiteRun = rng.choice(suiteRuns, size=(suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "  currentRuntimes1 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "  currentRuntimes2 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "\n",
    "  #Bulk selection\n",
    "  tmp1 = allRuntimes1[currentInstanceRun, currentSuiteRun, currentRuntimes1]\n",
    "  tmp1 = np.stack(tmp1, axis=3).reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "  tmp2 = allRuntimes2[currentInstanceRun, currentSuiteRun, currentRuntimes2]\n",
    "  tmp2 = np.stack(tmp2, axis=3).reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "\n",
    "  # Get median for both lists\n",
    "  med1 = np.median(tmp1, axis=1)\n",
    "  med2 = np.median(tmp2, axis=1)\n",
    "  R = med2/med1\n",
    "  R.sort()\n",
    "\n",
    "  CIsmall = 1 # 99% confidence interval\n",
    "  small = int((numberOfSamples * CIsmall) / 100 / 2)\n",
    "  if small == 0: small = 1\n",
    "  minSmall = R[small-1]\n",
    "  minSmall = (minSmall - 1) * 100\n",
    "  maxSmall = R[numberOfSamples-small-1]\n",
    "  maxSmall = (maxSmall - 1) * 100\n",
    "  instability = maxSmall - minSmall\n",
    "  return minSmall, maxSmall, instability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def compareVersionsForBenchmark(fnName: str):\n",
    "    foundBenchmark = df[df[\"package.BenchmarkFunction\"] == fnName]\n",
    "    runtimes1 = foundBenchmark[foundBenchmark[\"Version\"] == 1]\n",
    "    runtimes2 = foundBenchmark[foundBenchmark[\"Version\"] == 2]\n",
    "    assert runtimes1.shape[0] == 45\n",
    "    assert runtimes2.shape[0] == 45\n",
    "    median1 = runtimes1['sec/op'].median()\n",
    "    median2 = runtimes2['sec/op'].median()\n",
    "    change = ((median2/median1) - 1) * 100\n",
    "    minci, maxci, instability = bootstrap(runtimes1, runtimes2)\n",
    "    assert maxci > change\n",
    "    assert change > minci\n",
    "    print(f\"[{fnName}] performance change: {change:.2f}% [{minci:.2f} - {maxci:.2f}] ({instability:.2f}%)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[database.BenchmarkGet] performance change: -1.49% [-2.73 - 2.16] (4.89%)\n",
      "[database.BenchmarkGetGenerics] performance change: -0.55% [-4.03 - 3.58] (7.62%)\n",
      "[database.BenchmarkPut] performance change: 0.15% [-3.73 - 2.76] (6.49%)\n",
      "[database.BenchmarkRawGet] performance change: -0.11% [-4.00 - 1.13] (5.13%)\n",
      "[database.BenchmarkRawValues] performance change: 0.07% [-0.40 - 3.99] (4.39%)\n",
      "[database.BenchmarkValues] performance change: -0.03% [-1.16 - 1.26] (2.42%)\n",
      "[database.BenchmarkValuesGenerics] performance change: 0.28% [-1.92 - 1.64] (3.56%)\n",
      "[service.BenchmarkHandlerCreateBooking] performance change: 0.84% [-1.61 - 2.85] (4.46%)\n",
      "[service.BenchmarkHandlerGetBookings] performance change: -0.21% [-1.65 - 1.43] (3.08%)\n",
      "[service.BenchmarkHandlerGetDestinations] performance change: -0.10% [-2.28 - 1.30] (3.58%)\n",
      "[service.BenchmarkHandlerGetFlight] performance change: 0.88% [-2.28 - 8.15] (10.43%)\n",
      "[service.BenchmarkHandlerGetFlightSeats] performance change: -0.76% [-2.30 - 2.58] (4.88%)\n",
      "[service.BenchmarkHandlerGetFlights] performance change: 0.35% [-2.68 - 2.61] (5.29%)\n",
      "[service.BenchmarkHandlerGetFlightsQuery] performance change: 0.58% [-5.89 - 2.29] (8.18%)\n",
      "[service.BenchmarkRequestFlight] performance change: 61949.39% [59840.91 - 62355.47] (2514.56%)\n",
      "[service.BenchmarkRequestFlights] performance change: 332.73% [274.47 - 338.56] (64.09%)\n",
      "[service.BenchmarkRequestFlightsQuery] performance change: 69.41% [67.06 - 74.44] (7.37%)\n",
      "[service.BenchmarkRequestSeats] performance change: 420.10% [410.47 - 425.34] (14.88%)\n"
     ]
    }
   ],
   "source": [
    "columnNames = [\"R-S-I\", \"package.BenchmarkFunction\", \"Version\", \"Directory\", \"Iterations\", \"sec/op\", \"B/op\", \"allocs/op\"]\n",
    "\n",
    "#df = pd.read_csv(\"../results/fbs/mb-main-perf-issue-clean-path-2022-08-23T18:23:34+02:00.csv\", names=columnNames)\n",
    "df = pd.read_csv(\"../results/fbs/mb-main-perf-issue-request-id-2022-08-23T19:44:15+02:00.csv\", names=columnNames)\n",
    "functionNames = df[\"package.BenchmarkFunction\"].unique()\n",
    "functionNames.sort()\n",
    "\n",
    "for fnName in functionNames:\n",
    "    compareVersionsForBenchmark(fnName)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}