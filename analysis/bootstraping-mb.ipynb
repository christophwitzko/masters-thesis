{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "\n",
    "def bootstrap(perfRuntimes1: pd.DataFrame, perfRuntimes2: pd.DataFrame):\n",
    "  numberOfIterations = 5\n",
    "  instanceRunsNumber = 3\n",
    "  instanceRuns = range(1, instanceRunsNumber)\n",
    "  suiteRunsNumber = 3\n",
    "  suiteRuns = range(1,suiteRunsNumber)\n",
    "  numberOfSamples = 10000\n",
    "  allRuntimes1 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "  allRuntimes2 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "\n",
    "  for instanceRun in instanceRuns:\n",
    "    for suiteRun in suiteRuns:\n",
    "        prefix = f\"{instanceRun}-{suiteRun}-\"\n",
    "        allRuntimes1[instanceRun][suiteRun] = perfRuntimes1.loc[(perfRuntimes1['R-S-I'].str.startswith(prefix)),'sec/op'].to_numpy()\n",
    "        allRuntimes2[instanceRun][suiteRun] = perfRuntimes2.loc[(perfRuntimes2['R-S-I'].str.startswith(prefix)),'sec/op'].to_numpy()\n",
    "\n",
    "  #Generate random arrays\n",
    "  currentInstanceRun = rng.choice(instanceRuns, size=(instanceRunsNumber, numberOfSamples))\n",
    "  currentSuiteRun = rng.choice(suiteRuns, size=(suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "  currentRuntimes1 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "  currentRuntimes2 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "\n",
    "  #Bulk selection\n",
    "  tmp1 = allRuntimes1[currentInstanceRun, currentSuiteRun, currentRuntimes1]\n",
    "  tmp1 = np.stack(tmp1, axis=3).reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "  tmp2 = allRuntimes2[currentInstanceRun, currentSuiteRun, currentRuntimes2]\n",
    "  tmp2 = np.stack(tmp2, axis=3).reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "\n",
    "  # Get median for both lists\n",
    "  med1 = np.median(tmp1, axis=1)\n",
    "  med2 = np.median(tmp2, axis=1)\n",
    "  R = med2/med1\n",
    "  R.sort()\n",
    "\n",
    "  CIsmall = 1 # 99% confidence interval\n",
    "  small = int((numberOfSamples * CIsmall) / 100 / 2)\n",
    "  if small == 0: small = 1\n",
    "  minSmall = R[small-1]\n",
    "  minSmall = (minSmall - 1) * 100\n",
    "  maxSmall = R[numberOfSamples-small-1]\n",
    "  maxSmall = (maxSmall - 1) * 100\n",
    "  instability = maxSmall - minSmall\n",
    "  return minSmall, maxSmall, instability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def compareVersionsForBenchmark(fnName: str):\n",
    "    foundBenchmark = df[df[\"package.BenchmarkFunction\"] == fnName]\n",
    "    runtimes1 = foundBenchmark[foundBenchmark[\"Version\"] == 1]\n",
    "    runtimes2 = foundBenchmark[foundBenchmark[\"Version\"] == 2]\n",
    "    assert runtimes1.shape[0] == 45\n",
    "    assert runtimes2.shape[0] == 45\n",
    "    median1 = runtimes1['sec/op'].median()\n",
    "    median2 = runtimes2['sec/op'].median()\n",
    "    change = ((median2/median1) - 1) * 100\n",
    "    minci, maxci, instability = bootstrap(runtimes1, runtimes2)\n",
    "    assert maxci > change\n",
    "    assert change > minci\n",
    "    print(f\"[{fnName}] performance change: {change:.2f}% [{minci:.2f} - {maxci:.2f}] ({instability:.2f}%)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[service.BenchmarkHandlerCreateBooking] performance change: 0.54% [-0.73 - 3.58] (4.31%)\n",
      "[service.BenchmarkHandlerGetFlightSeats] performance change: 0.30% [-1.47 - 1.40] (2.87%)\n",
      "[service.BenchmarkRequestDestinations] performance change: 0.27% [-1.00 - 2.15] (3.15%)\n",
      "[service.BenchmarkRequestFlightsQuery] performance change: 3.41% [2.12 - 5.01] (2.90%)\n"
     ]
    }
   ],
   "source": [
    "columnNames = [\"R-S-I\", \"package.BenchmarkFunction\", \"Version\", \"Directory\", \"Iterations\", \"sec/op\", \"B/op\", \"allocs/op\"]\n",
    "\n",
    "df = pd.read_csv(\"../results/fbs/mb-main-perf-issue-clean-path-2022-09-23T13:30:10+02:00/combined.csv\", names=columnNames)\n",
    "# df = pd.read_csv(\"../results/fbs/mb-main-perf-issue-request-id-2022-09-23T13:01:00+02:00/combined.csv\", names=columnNames)\n",
    "# df = pd.read_csv(\"../results/fbs/mb-main-main-2022-09-23T12:10:26+02:00/combined.csv\", names=columnNames)\n",
    "functionNames = df[\"package.BenchmarkFunction\"].unique()\n",
    "functionNames.sort()\n",
    "\n",
    "for fnName in functionNames:\n",
    "    compareVersionsForBenchmark(fnName)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
